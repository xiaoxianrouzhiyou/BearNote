### 无约束最优化方法

1. 分类

   * 直接法：只用函数值，收敛速度一般较慢
   * 间接法：要用到导数，甚至Hesse，一般收敛速度较快，但求导又是很复杂
   * 尽可能用间接法

2. 最速下降法

   * 沿目标函数负梯度方向，最优步长

   * ls=linear search
     $$
     X_{k+1}=ls(X_k,-\nabla f(X_k))
     $$

   * 对于正定二次函数的公式
     $$
     f(X)=\frac{1}{2}X^TAX+b^TX+c\\
     关于X求梯度g(X)=AX+b\\
     X_{k+1}=X_k-t_kg_k\\
     g(X_{k+1})^Tg_k=0\\
     X_{k+1}=X_k-\frac{g_k^Tg_k}{g_k^TAg_k}g_k
     $$

   * 相邻两个搜索方向是正交的

   * 优点：简单、每次迭代计算量小、占用内存量小、即使初始点不好也能收敛到局部极小值

   * 缺点：收敛速度慢，由于每次方向都垂直，锯齿状，当距离极小值近的时候收敛慢

3. 牛顿法

   * 有二阶导

   * 收敛速度快

   * 一维搜索牛顿切线法的推广

   * 用点Xk指向近似二次函数极小点的方向，步长为1，进行迭代
     $$
     泰勒展开，求梯度，令之为0\\
     P_k=-[\nabla^2f(X_k)]^{-1}\nabla f(X_k)
     $$

   * 对于正定二次函数，牛顿方向就是指向极小点的方向
   * 用牛顿法解目标函数为正定二次函数的无约束最优化问题，只需一次迭代就可得最优解
   * 缺点
     * 每次迭代不会是目标函数上升，但不能保证下降
     * Hesse矩阵非正定，牛顿搜索将失败
     * 因为P求解存在Hesse的拟，可能不存在
     * 计算Hesse复杂，占用很多机器内存
     * 非二次函数的话，不得保证求得最优解，但在初始点选取离最优解近时，收敛速度较快，太远则下降性很难保证

4. 修正牛顿法

   * 方向还是那个方向，步长用一维搜索法确定最优步长
   * 可以修改步长，因而对初始点的选择不再严格，但Hesse仍然占内存，而且可能不存在

5. 共轭方向法

   * 计算量小，收敛速度没有牛顿法快，但比最速下降法快得多        

   * A共轭，A正交
     $$
     P_1^TAP_2=0
     $$

   * A共轭方向组
     $$
     P_i为一组非零向量\\
     P_i^TAP_j=0
     $$

   * A取单位矩阵，共轭是正交的推广

   * 对于正定矩阵A，共轭方向组是线性无关的

   * 对于正定矩阵A，从任意点出发沿共轭方向组搜索，至多经过n轮迭代，可得最优解

   * 如果希望P1指向最优解，结束迭代
     $$
     X^*=X_1+t_1P_1\\
     解得P_0^TAP_1=0,共轭\\
     P_1=-\nabla f(X_1)+\frac{P_0^TA\nabla f(X_1)}{P_0^TAP_0}P_0
     $$

   * 

   * 二次终止性：经过有限次迭代能求出极小值，例如牛顿法对二次函数只经过一次迭代，最速下降法不是，共轭方向法、共轭梯度法、变尺度法是，一般收敛速度较快

6. 共轭梯度法

   * 每个共轭向量都依赖于迭代点处的负梯度构造出来的

   $$
   P_0=-\nabla f(X_0)\\
   P_{k+1}=-\nabla f(X_{k+1}+\lambda_kP_k)\\
   \lambda_k=\frac{\nabla f(X_{k+1})^TAP_k}{P_k^TAP_k}
   $$

   * 以上对非二次函数不方便，简化为
     $$
     \lambda_k=\frac{||\nabla f(X_{k+1})||^2}{||\nabla f(X_k)||^2}
     $$

   * lamba为0，变为最速下降法
   * 不涉及矩阵，存储小，适合高维优化问题
   * 不要求精确的直线搜索，可能导致迭代之后不再共轭，所以要把迭代出来的作为初始点重新迭代

7. 变尺度法

   * 牛顿法快，但是Hesse计算复杂占内存，找一种方法及快速收敛又摆脱Hesse

   * 用近似矩阵替换Hesse(G)的逆
     $$
     X_{k+1}=X_k-G_k^{-1}g_k\\
     X_{k+1}=X_k-t_kH_kg_k
     $$

   * H正定，保证P是下降方向，P与梯度g成钝角
     $$
     P_k=-H_kg_k\\
     g_k^TP_k=-g_k^TH_kg_k<0
     $$

   * 利用泰勒公式，得到H应满足的条件
     $$
     H_{k+1}(g_{k+1}-g_k)=(X_{k+1}-X_k)\\
     令y_k=g_{k+1}-g_k\\
     S_k=X_{k+1}-X_k
     $$

   * 迭代公式，根据E的不同算法分为多种方法
     $$
     H_{k+1}=H_k+E_k\\
     H_0=I（与最速下降法具有相同的第一迭代点）
     $$

     * DFP
       $$
       H_{k+1}=H_k+\frac{S_kS_k^T}{S_k^Ty_k}-\frac{H_ky_ky_k^TH_k}{y_k^TH_ky_k}
       $$

       * 也存在直线搜索不精确，导致某一轮的H奇异，可以重置初始点，重新迭代

     * BFGS
       $$
       H_{k+1}=H_k+\frac{1}{S^T_ky_k}[(1+\frac{y^T_kH_ky_k}{S^T_ky_k})S_kS_k^T-H_ky_kS^T_k-H_ky_kS^T_k-S_ky^T_kH_k]
       $$
       

       * 令beta为0，就是DFP
       * H不易变为奇异矩阵

8. 坐标轮换法

   * 直接法
   * 经过适当处理解决有约束最优问题
   * 每次沿一个方向搜索，其他分量不变，每个分量轮完，进入下一次
   * 优点：算法简单、计算量小
   * 缺点：计算效率低，特别是对高维问题

9. 单纯形法

   * 直接法

   * 在连续改变几何图形的过程中，逐步以目标函数值较小的顶点取代目标函数值最大的顶点

   * n维，构建顶点为n+1的单纯形，以X0为初始点，沿各坐标反向以步长t移动得n个顶点，得到各边向量线性无关，否则会使搜索范围局限在较低维的空间

   * 最差点的反对称方向
     $$
     计算最差点Xh之外各点的重心X_{n+1}=\frac{1}{n}(\sum^n_{i=0}X_i-X_H)\\
     反射点X_{n+2}=2X{n+1}-X_H
     $$

   * 向最好点缩边

   * 收敛。各点距离最好点一定近

   * 内存少，对变量不多精度要求不高的问题很简单，但是当变量个数变多，不十分有效

